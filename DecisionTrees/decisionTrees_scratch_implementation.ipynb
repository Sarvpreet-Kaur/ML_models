{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fefb85f9",
   "metadata": {},
   "source": [
    "## Decision Tree implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee6d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import matplotlib.pyplot as plt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432b5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from sklearn.datasets import load_iris;\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ff0084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                  5.1               3.5                1.4               0.2\n",
      "1                  4.9               3.0                1.4               0.2\n",
      "2                  4.7               3.2                1.3               0.2\n",
      "3                  4.6               3.1                1.5               0.2\n",
      "4                  5.0               3.6                1.4               0.2\n",
      "..                 ...               ...                ...               ...\n",
      "145                6.7               3.0                5.2               2.3\n",
      "146                6.3               2.5                5.0               1.9\n",
      "147                6.5               3.0                5.2               2.0\n",
      "148                6.2               3.4                5.4               2.3\n",
      "149                5.9               3.0                5.1               1.8\n",
      "\n",
      "[150 rows x 4 columns]\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "145    2\n",
      "146    2\n",
      "147    2\n",
      "148    2\n",
      "149    2\n",
      "Name: target, Length: 150, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris();\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "print(X)\n",
    "\n",
    "y = pd.Series(iris.target, name=\"target\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eecb5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845cc6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DecisionTree Classifier with Pruning\n",
    "'''\n",
    "gini: impurity score (how mixed the classes are).\n",
    "num_samples: number of training samples reaching this node.\n",
    "num_samples_per_class: count of each class at this node.\n",
    "predicted_class: majority class label (used if this node is a leaf).\n",
    "feature_index: which feature was chosen for splitting.\n",
    "threshold: the cut-off value for the split.\n",
    "left / right: child nodes (subtrees).\n",
    "'''\n",
    "# For each node in a tree\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, gini, num_samples, num_sample_per_class, predicted_class):\n",
    "        self.gini = gini\n",
    "        self.num_samples = num_samples\n",
    "        self.num_samples_per_class = num_sample_per_class\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.left = None\n",
    "        self.right = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f65c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassification:\n",
    "    '''\n",
    "    max_depth: maximum depth of the tree (prevents infinite growth).\n",
    "    min_samples_split: minimum samples required to allow a split.\n",
    "    min_impurity_decrease: only split if Gini impurity decreases at least this much.\n",
    "    '''\n",
    "    def __init__(self, max_depth = None, min_samples_split = 2, min_impurity_decrease=1e-7):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        \n",
    "    '''\n",
    "    (np.sum(y==c)/m) ** 2 for c in np.unique(y)) -> for calculating probability\n",
    "    (np.sum(y==c)/m) finding number of features belonging to that particular class and divide by total to find prob.\n",
    "    Sqauring each prob. and summing it up\n",
    "    '''  \n",
    "    def _gini(self, y):\n",
    "        m = len(y)\n",
    "        if m==0:\n",
    "            return 0\n",
    "        return 1.0- sum((np.sum(y==c)/m) ** 2 for c in np.unique(y))\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        m,n = X.shape\n",
    "        \n",
    "        #if few samples -> no further split\n",
    "        if m < self.min_samples_split:\n",
    "            return None, None, 0\n",
    "        \n",
    "        # counting samples per class at current node\n",
    "        num_parent = [np.sum(y==c) for c in np.unique(y)]\n",
    "        \n",
    "        #computing parent gini\n",
    "        best_gini = self._gini(y) \n",
    "        \n",
    "        best_idx, best_thr = None, None\n",
    "        best_impurity_decrease = 0\n",
    "        \n",
    "        #for each feature\n",
    "        '''\n",
    "        Step 1: zip(X[:, idx], y)\n",
    "            X[:, idx] â†’ all values of feature idx.\n",
    "            y â†’ all class labels.\n",
    "            zip pairs each feature value with its corresponding label.\n",
    "            \n",
    "            list(zip(X[:, idx], y))\n",
    "            â†’ [(5.1, 0), (4.9, 0), (6.2, 1)]\n",
    "            \n",
    "        Step 2: sorted(zip(X[:, idx], y))\n",
    "            Sorts these pairs by the feature value (first element of each tuple).\n",
    "            This ensures we're evaluating potential split thresholds in ascending order of the feature values.\n",
    "            Example:\n",
    "            sorted([(5.1, 0), (4.9, 0), (6.2, 1)])\n",
    "            â†’ [(4.9, 0), (5.1, 0), (6.2, 1)]\n",
    "\n",
    "        Step 3: zip(*sorted(...))\n",
    "            The * unpacks the sorted list of tuples into two separate sequences.\n",
    "            zip(*) then groups the first elements together and the second elements together.\n",
    "            ğŸ‘‰ Example:\n",
    "            zip(*[(4.9, 0), (5.1, 0), (6.2, 1)])\n",
    "            â†’ ( (4.9, 5.1, 6.2), (0, 0, 1) )\n",
    "        '''\n",
    "        for idx in range(n):\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "            # class counts on left side \n",
    "            num_left = [0]*len(np.unique(y))\n",
    "            num_right = num_parent.copy()\n",
    "            \n",
    "            for i in range(1, m):\n",
    "                c = classes[i-1]\n",
    "                class_idx = list(np.unique(y)).index(c)\n",
    "                num_left[class_idx] += 1\n",
    "                num_right[class_idx] -= 1\n",
    "                \n",
    "                #Computing gini for left index\n",
    "                gini_left = 1.0 - sum((num_left[x]/i)**2 for x in range(len(np.unique(y))))\n",
    "                gini_right = 1.0 - sum((num_right[x]/(m-i)) ** 2 for x in range(len(np.unique(y))))\n",
    "                \n",
    "                gini = (i*gini_left + (m-i)*gini_right)/m\n",
    "                \n",
    "                impurity_decrease = best_gini - gini\n",
    "                \n",
    "                '''\n",
    "                If two consecutive sorted feature values are equal, there is no meaningful threshold\n",
    "                between them (would produce identical left/right), so skip this candidate.\n",
    "                '''\n",
    "                \n",
    "                if(thresholds[i] == thresholds[i-1]):\n",
    "                    continue\n",
    "                \n",
    "                '''\n",
    "                If this candidate split gives a larger impurity decrease than any previous candidate\n",
    "                for any feature, update the best feature best_idx, the chosen threshold best_thr (midpoint),\n",
    "                and the best_impurity_decrease.\n",
    "                '''\n",
    "                if impurity_decrease > best_impurity_decrease:\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i-1])/2\n",
    "                    best_impurity_decrease = impurity_decrease\n",
    "                    \n",
    "                '''\n",
    "                If the best gain is smaller than min_impurity_decrease (another pre-pruning rule), we decline\n",
    "                to split (return None) â€” otherwise return the chosen feature index, threshold, and the impurity gain.\n",
    "                '''\n",
    "        if best_impurity_decrease < self.min_impurity_decrease:\n",
    "            return None, None, 0\n",
    "        \n",
    "        return best_idx, best_thr, best_impurity_decrease\n",
    "            \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        # Count samples per class\n",
    "        num_samples_per_class = [np.sum(y==i) for i in np.unique(y)]\n",
    "        # Choose majority class at this node\n",
    "        predicted_class = np.unique(y)[np.argmax(num_samples_per_class)]\n",
    "        \n",
    "        # Creating tree node\n",
    "        node = DecisionTreeNode(\n",
    "            gini = self._gini(y),\n",
    "            num_samples = len(y),\n",
    "            num_sample_per_class = num_samples_per_class,\n",
    "            predicted_class = predicted_class,\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        This is the core recursive splitting step of the tree. If the node is allowed to\n",
    "        split (depth check), it asks _best_split for the best feature and threshold; if a valid \n",
    "        split is found it partitions the data into left/right subsets, attaches the feature/threshold\n",
    "        to the current node, and recursively grows left and right child nodes (incrementing depth).\n",
    "        If no valid split is found, the node remains a leaf.\n",
    "        '''\n",
    "        \n",
    "        if depth < self.max_depth:\n",
    "            idx, thr, impurity = self._best_split(X, y)\n",
    "            if idx is not None:\n",
    "                #boolean mask that selects rows \n",
    "                indices_left = X[:, idx] < thr\n",
    "                \n",
    "                X_left, y_left = np.array(X[indices_left]), np.array(y[indices_left])\n",
    "                X_right, y_right = np.array(X[~indices_left]), np.array(y[~indices_left])\n",
    "\n",
    "                \n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left, depth+1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth+1)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        self.n_classes = len(set(y))\n",
    "        self.n_features = X.shape[1]\n",
    "        self.tree_ = self._grow_tree(X, y)\n",
    "        \n",
    "    def _predict(self, inputs):\n",
    "        #Root node of trained decision tree\n",
    "        node = self.tree_\n",
    "        \n",
    "        while node.left:\n",
    "            '''\n",
    "            The input sample inputs is an array of feature values for a single data point.\n",
    "            This line checks: does this sample's value for the chosen feature fall on the left\n",
    "            side or the right side of the split\n",
    "            '''\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48d5dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pruned = DecisionTreeClassification(max_depth = 5, min_samples_split = 5, min_impurity_decrease = 1e-3)\n",
    "tree_pruned.fit(X_train.values, y_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b03f2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "y_pred = tree_pruned.predict(X_test.values)\n",
    "accuracy = np.mean(y_pred == y_test.values)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d9447",
   "metadata": {},
   "source": [
    "## Decision Tree Built successfully"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
